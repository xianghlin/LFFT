# Frequency Transformer with Local Feature Enhancement for Vehicle Re-Identification
## Abstract
With the rapid development of intelligent security systems, the demand for vehicle re-identification has surged exponentially. Vehicle re-identification involves recognizing the same vehicle across different camera perspectives, necessitating robust local feature processing. While transformers have shown promising results in this field, their inherent self-attention mechanism tends to dilute high-frequency texture details, hindering local feature extraction. Additionally, challenges such as occlusion and misalignment can lead to information loss and noise introduction, reducing re-identification accuracy. To address these issues, we introduce the Frequency Transformer with Local Feature Enhancement (LFFT). The proposed framework comprises a Frequency Layer and a Jigsaw Select Patches Module (JSPM). The Frequency Layer enhances the weights of high-frequency component features using Fast Fourier Transform to improve local feature extraction at the lower layers. Meanwhile, the Attention Layers at the higher layers continue to extract global features. The JSPM incorporates discriminative patches obtained from the Attention Layers into randomly shuffled and reorganized groups, enhancing the global discriminative capability of local features. This method does not increase the computational and model complexity. Experimental evaluations on two vehicle re-identification datasets, VeRi-776 and VehicleID, demonstrate the effectiveness of our method compared to recent approaches.
## Requirements
### Environment
```
torch 1.6.0
torchvision 0.7.0
timm 0.3.2
numpy 1.21.6
cuda 10.1
yacs 0.1.6
opencv-python 4.7.0.42
```
### DataSets
vehicle datasets [VehicleID](https://www.pkuml.org/resources/pku-vehicleid.html), [VeRi-776](https://github.com/JDAI-CV/VeRidataset), Then unzip them and rename them under the directory like
```
data
├── VehicleID_V1.0
│   └── images ..
└── VeRi
    └── images ..
```
## Methods
### Architecture
The proposed Frequency Transformer with Local Feature Enhancement consists of two distinct stages. The initial $\lambda$ layers are designated as Frequency Layers, and the subsequent L- $\lambda$ -1 layers are Attention Layers. The last layer contains two independent modules: one for extracting global features, and the other incorporating the Jigsaw and Select Patches Module (JSPM) for extracting local features. The red tokens are the most discriminative patches.

![architectue_new](https://github.com/user-attachments/assets/b102ebe5-fb02-42c5-aa70-ebefa43056f6)
### Frequency Layer
(a) Architecture of a Frequency Layer. (b) shows the structure of the Local Feature Enhancement(LFE) Block from Frequency space.

<img src="https://github.com/user-attachments/assets/62e6589c-9266-482e-b8a4-89369e67bd7b" width="500x">

### JSPM
Integrate the attentional weight matrices generated by the MHSA and select the most discriminative patches indexes from them.

<img src="https://github.com/user-attachments/assets/315c1c3b-6093-4fc8-bc18-d50f1949d6cd" width="500x">

The local feature map is obtained through random shuffling and reorganization using the Jigsaw Select Patches Module(JSPM)

<img src="https://github.com/user-attachments/assets/8a363bc0-4b33-43cc-80a0-85882aeca0a7" width="500x">

